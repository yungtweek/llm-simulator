// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.11
// 	protoc        v5.29.3
// source: llm.proto

package llmv1

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

type RequestMeta struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	RequestId     string                 `protobuf:"bytes,1,opt,name=request_id,json=requestId,proto3" json:"request_id,omitempty"`
	TraceId       string                 `protobuf:"bytes,2,opt,name=trace_id,json=traceId,proto3" json:"trace_id,omitempty"`
	SessionId     string                 `protobuf:"bytes,3,opt,name=session_id,json=sessionId,proto3" json:"session_id,omitempty"`
	UserId        string                 `protobuf:"bytes,4,opt,name=user_id,json=userId,proto3" json:"user_id,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *RequestMeta) Reset() {
	*x = RequestMeta{}
	mi := &file_llm_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RequestMeta) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RequestMeta) ProtoMessage() {}

func (x *RequestMeta) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RequestMeta.ProtoReflect.Descriptor instead.
func (*RequestMeta) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{0}
}

func (x *RequestMeta) GetRequestId() string {
	if x != nil {
		return x.RequestId
	}
	return ""
}

func (x *RequestMeta) GetTraceId() string {
	if x != nil {
		return x.TraceId
	}
	return ""
}

func (x *RequestMeta) GetSessionId() string {
	if x != nil {
		return x.SessionId
	}
	return ""
}

func (x *RequestMeta) GetUserId() string {
	if x != nil {
		return x.UserId
	}
	return ""
}

type ChatMessage struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Role          string                 `protobuf:"bytes,1,opt,name=role,proto3" json:"role,omitempty"`
	Content       string                 `protobuf:"bytes,2,opt,name=content,proto3" json:"content,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ChatMessage) Reset() {
	*x = ChatMessage{}
	mi := &file_llm_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ChatMessage) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ChatMessage) ProtoMessage() {}

func (x *ChatMessage) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ChatMessage.ProtoReflect.Descriptor instead.
func (*ChatMessage) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{1}
}

func (x *ChatMessage) GetRole() string {
	if x != nil {
		return x.Role
	}
	return ""
}

func (x *ChatMessage) GetContent() string {
	if x != nil {
		return x.Content
	}
	return ""
}

type ChatCompletionRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Correlation / metrics tags
	Meta *RequestMeta `protobuf:"bytes,1,opt,name=meta,proto3" json:"meta,omitempty"`
	// Model / prompt
	Model        string `protobuf:"bytes,2,opt,name=model,proto3" json:"model,omitempty"`
	SystemPrompt string `protobuf:"bytes,3,opt,name=system_prompt,json=systemPrompt,proto3" json:"system_prompt,omitempty"`
	UserPrompt   string `protobuf:"bytes,4,opt,name=user_prompt,json=userPrompt,proto3" json:"user_prompt,omitempty"`
	// Optional context as a list of prior messages
	Context []*ChatMessage `protobuf:"bytes,5,rep,name=context,proto3" json:"context,omitempty"`
	// Sampling params (mock can ignore most except max_tokens)
	Temperature   float64 `protobuf:"fixed64,6,opt,name=temperature,proto3" json:"temperature,omitempty"`
	MaxTokens     int32   `protobuf:"varint,7,opt,name=max_tokens,json=maxTokens,proto3" json:"max_tokens,omitempty"`
	TopP          float64 `protobuf:"fixed64,8,opt,name=top_p,json=topP,proto3" json:"top_p,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ChatCompletionRequest) Reset() {
	*x = ChatCompletionRequest{}
	mi := &file_llm_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ChatCompletionRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ChatCompletionRequest) ProtoMessage() {}

func (x *ChatCompletionRequest) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ChatCompletionRequest.ProtoReflect.Descriptor instead.
func (*ChatCompletionRequest) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{2}
}

func (x *ChatCompletionRequest) GetMeta() *RequestMeta {
	if x != nil {
		return x.Meta
	}
	return nil
}

func (x *ChatCompletionRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *ChatCompletionRequest) GetSystemPrompt() string {
	if x != nil {
		return x.SystemPrompt
	}
	return ""
}

func (x *ChatCompletionRequest) GetUserPrompt() string {
	if x != nil {
		return x.UserPrompt
	}
	return ""
}

func (x *ChatCompletionRequest) GetContext() []*ChatMessage {
	if x != nil {
		return x.Context
	}
	return nil
}

func (x *ChatCompletionRequest) GetTemperature() float64 {
	if x != nil {
		return x.Temperature
	}
	return 0
}

func (x *ChatCompletionRequest) GetMaxTokens() int32 {
	if x != nil {
		return x.MaxTokens
	}
	return 0
}

func (x *ChatCompletionRequest) GetTopP() float64 {
	if x != nil {
		return x.TopP
	}
	return 0
}

type ChatCompletionResponse struct {
	state            protoimpl.MessageState `protogen:"open.v1"`
	OutputText       string                 `protobuf:"bytes,1,opt,name=output_text,json=outputText,proto3" json:"output_text,omitempty"`
	FinishReason     string                 `protobuf:"bytes,2,opt,name=finish_reason,json=finishReason,proto3" json:"finish_reason,omitempty"`
	PromptTokens     int32                  `protobuf:"varint,3,opt,name=prompt_tokens,json=promptTokens,proto3" json:"prompt_tokens,omitempty"`
	CompletionTokens int32                  `protobuf:"varint,4,opt,name=completion_tokens,json=completionTokens,proto3" json:"completion_tokens,omitempty"`
	TotalTokens      int32                  `protobuf:"varint,5,opt,name=total_tokens,json=totalTokens,proto3" json:"total_tokens,omitempty"`
	LatencyMs        int64                  `protobuf:"varint,6,opt,name=latency_ms,json=latencyMs,proto3" json:"latency_ms,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *ChatCompletionResponse) Reset() {
	*x = ChatCompletionResponse{}
	mi := &file_llm_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ChatCompletionResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ChatCompletionResponse) ProtoMessage() {}

func (x *ChatCompletionResponse) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ChatCompletionResponse.ProtoReflect.Descriptor instead.
func (*ChatCompletionResponse) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{3}
}

func (x *ChatCompletionResponse) GetOutputText() string {
	if x != nil {
		return x.OutputText
	}
	return ""
}

func (x *ChatCompletionResponse) GetFinishReason() string {
	if x != nil {
		return x.FinishReason
	}
	return ""
}

func (x *ChatCompletionResponse) GetPromptTokens() int32 {
	if x != nil {
		return x.PromptTokens
	}
	return 0
}

func (x *ChatCompletionResponse) GetCompletionTokens() int32 {
	if x != nil {
		return x.CompletionTokens
	}
	return 0
}

func (x *ChatCompletionResponse) GetTotalTokens() int32 {
	if x != nil {
		return x.TotalTokens
	}
	return 0
}

func (x *ChatCompletionResponse) GetLatencyMs() int64 {
	if x != nil {
		return x.LatencyMs
	}
	return 0
}

type ChatCompletionChunkResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Responses-style event type
	// e.g. "output_text.delta", "output_text.done"
	Type string `protobuf:"bytes,1,opt,name=type,proto3" json:"type,omitempty"`
	// Streaming payload
	Text string `protobuf:"bytes,2,opt,name=text,proto3" json:"text,omitempty"` // delta text for *.delta events
	// Completion metadata (set on done event)
	FinishReason     string `protobuf:"bytes,3,opt,name=finish_reason,json=finishReason,proto3" json:"finish_reason,omitempty"`
	Index            int32  `protobuf:"varint,4,opt,name=index,proto3" json:"index,omitempty"`
	PromptTokens     int32  `protobuf:"varint,5,opt,name=prompt_tokens,json=promptTokens,proto3" json:"prompt_tokens,omitempty"`
	CompletionTokens int32  `protobuf:"varint,6,opt,name=completion_tokens,json=completionTokens,proto3" json:"completion_tokens,omitempty"`
	TotalTokens      int32  `protobuf:"varint,7,opt,name=total_tokens,json=totalTokens,proto3" json:"total_tokens,omitempty"`
	LatencyMs        int64  `protobuf:"varint,8,opt,name=latency_ms,json=latencyMs,proto3" json:"latency_ms,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *ChatCompletionChunkResponse) Reset() {
	*x = ChatCompletionChunkResponse{}
	mi := &file_llm_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ChatCompletionChunkResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ChatCompletionChunkResponse) ProtoMessage() {}

func (x *ChatCompletionChunkResponse) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ChatCompletionChunkResponse.ProtoReflect.Descriptor instead.
func (*ChatCompletionChunkResponse) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{4}
}

func (x *ChatCompletionChunkResponse) GetType() string {
	if x != nil {
		return x.Type
	}
	return ""
}

func (x *ChatCompletionChunkResponse) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

func (x *ChatCompletionChunkResponse) GetFinishReason() string {
	if x != nil {
		return x.FinishReason
	}
	return ""
}

func (x *ChatCompletionChunkResponse) GetIndex() int32 {
	if x != nil {
		return x.Index
	}
	return 0
}

func (x *ChatCompletionChunkResponse) GetPromptTokens() int32 {
	if x != nil {
		return x.PromptTokens
	}
	return 0
}

func (x *ChatCompletionChunkResponse) GetCompletionTokens() int32 {
	if x != nil {
		return x.CompletionTokens
	}
	return 0
}

func (x *ChatCompletionChunkResponse) GetTotalTokens() int32 {
	if x != nil {
		return x.TotalTokens
	}
	return 0
}

func (x *ChatCompletionChunkResponse) GetLatencyMs() int64 {
	if x != nil {
		return x.LatencyMs
	}
	return 0
}

var File_llm_proto protoreflect.FileDescriptor

const file_llm_proto_rawDesc = "" +
	"\n" +
	"\tllm.proto\x12\x06llm.v1\"\x7f\n" +
	"\vRequestMeta\x12\x1d\n" +
	"\n" +
	"request_id\x18\x01 \x01(\tR\trequestId\x12\x19\n" +
	"\btrace_id\x18\x02 \x01(\tR\atraceId\x12\x1d\n" +
	"\n" +
	"session_id\x18\x03 \x01(\tR\tsessionId\x12\x17\n" +
	"\auser_id\x18\x04 \x01(\tR\x06userId\";\n" +
	"\vChatMessage\x12\x12\n" +
	"\x04role\x18\x01 \x01(\tR\x04role\x12\x18\n" +
	"\acontent\x18\x02 \x01(\tR\acontent\"\xa1\x02\n" +
	"\x15ChatCompletionRequest\x12'\n" +
	"\x04meta\x18\x01 \x01(\v2\x13.llm.v1.RequestMetaR\x04meta\x12\x14\n" +
	"\x05model\x18\x02 \x01(\tR\x05model\x12#\n" +
	"\rsystem_prompt\x18\x03 \x01(\tR\fsystemPrompt\x12\x1f\n" +
	"\vuser_prompt\x18\x04 \x01(\tR\n" +
	"userPrompt\x12-\n" +
	"\acontext\x18\x05 \x03(\v2\x13.llm.v1.ChatMessageR\acontext\x12 \n" +
	"\vtemperature\x18\x06 \x01(\x01R\vtemperature\x12\x1d\n" +
	"\n" +
	"max_tokens\x18\a \x01(\x05R\tmaxTokens\x12\x13\n" +
	"\x05top_p\x18\b \x01(\x01R\x04topP\"\xf2\x01\n" +
	"\x16ChatCompletionResponse\x12\x1f\n" +
	"\voutput_text\x18\x01 \x01(\tR\n" +
	"outputText\x12#\n" +
	"\rfinish_reason\x18\x02 \x01(\tR\ffinishReason\x12#\n" +
	"\rprompt_tokens\x18\x03 \x01(\x05R\fpromptTokens\x12+\n" +
	"\x11completion_tokens\x18\x04 \x01(\x05R\x10completionTokens\x12!\n" +
	"\ftotal_tokens\x18\x05 \x01(\x05R\vtotalTokens\x12\x1d\n" +
	"\n" +
	"latency_ms\x18\x06 \x01(\x03R\tlatencyMs\"\x94\x02\n" +
	"\x1bChatCompletionChunkResponse\x12\x12\n" +
	"\x04type\x18\x01 \x01(\tR\x04type\x12\x12\n" +
	"\x04text\x18\x02 \x01(\tR\x04text\x12#\n" +
	"\rfinish_reason\x18\x03 \x01(\tR\ffinishReason\x12\x14\n" +
	"\x05index\x18\x04 \x01(\x05R\x05index\x12#\n" +
	"\rprompt_tokens\x18\x05 \x01(\x05R\fpromptTokens\x12+\n" +
	"\x11completion_tokens\x18\x06 \x01(\x05R\x10completionTokens\x12!\n" +
	"\ftotal_tokens\x18\a \x01(\x05R\vtotalTokens\x12\x1d\n" +
	"\n" +
	"latency_ms\x18\b \x01(\x03R\tlatencyMs2\xbb\x01\n" +
	"\n" +
	"LlmService\x12O\n" +
	"\x0eChatCompletion\x12\x1d.llm.v1.ChatCompletionRequest\x1a\x1e.llm.v1.ChatCompletionResponse\x12\\\n" +
	"\x14ChatCompletionStream\x12\x1d.llm.v1.ChatCompletionRequest\x1a#.llm.v1.ChatCompletionChunkResponse0\x01B Z\x1ellm-simulator/gen/llm/v1;llmv1b\x06proto3"

var (
	file_llm_proto_rawDescOnce sync.Once
	file_llm_proto_rawDescData []byte
)

func file_llm_proto_rawDescGZIP() []byte {
	file_llm_proto_rawDescOnce.Do(func() {
		file_llm_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_llm_proto_rawDesc), len(file_llm_proto_rawDesc)))
	})
	return file_llm_proto_rawDescData
}

var file_llm_proto_msgTypes = make([]protoimpl.MessageInfo, 5)
var file_llm_proto_goTypes = []any{
	(*RequestMeta)(nil),                 // 0: llm.v1.RequestMeta
	(*ChatMessage)(nil),                 // 1: llm.v1.ChatMessage
	(*ChatCompletionRequest)(nil),       // 2: llm.v1.ChatCompletionRequest
	(*ChatCompletionResponse)(nil),      // 3: llm.v1.ChatCompletionResponse
	(*ChatCompletionChunkResponse)(nil), // 4: llm.v1.ChatCompletionChunkResponse
}
var file_llm_proto_depIdxs = []int32{
	0, // 0: llm.v1.ChatCompletionRequest.meta:type_name -> llm.v1.RequestMeta
	1, // 1: llm.v1.ChatCompletionRequest.context:type_name -> llm.v1.ChatMessage
	2, // 2: llm.v1.LlmService.ChatCompletion:input_type -> llm.v1.ChatCompletionRequest
	2, // 3: llm.v1.LlmService.ChatCompletionStream:input_type -> llm.v1.ChatCompletionRequest
	3, // 4: llm.v1.LlmService.ChatCompletion:output_type -> llm.v1.ChatCompletionResponse
	4, // 5: llm.v1.LlmService.ChatCompletionStream:output_type -> llm.v1.ChatCompletionChunkResponse
	4, // [4:6] is the sub-list for method output_type
	2, // [2:4] is the sub-list for method input_type
	2, // [2:2] is the sub-list for extension type_name
	2, // [2:2] is the sub-list for extension extendee
	0, // [0:2] is the sub-list for field type_name
}

func init() { file_llm_proto_init() }
func file_llm_proto_init() {
	if File_llm_proto != nil {
		return
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_llm_proto_rawDesc), len(file_llm_proto_rawDesc)),
			NumEnums:      0,
			NumMessages:   5,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_llm_proto_goTypes,
		DependencyIndexes: file_llm_proto_depIdxs,
		MessageInfos:      file_llm_proto_msgTypes,
	}.Build()
	File_llm_proto = out.File
	file_llm_proto_goTypes = nil
	file_llm_proto_depIdxs = nil
}
